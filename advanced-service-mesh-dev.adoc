= Lab 3 - サービスメッシュの高度なデプロイメント
:experimental:

このラボでは、サービスメッシュの高度な使用例を学びます。ラボでは以下の機能を紹介します:

* フォルトインジェクション (Fault Injection)
* トラフィックシフト (Traffic Shifting)
* サーキットブレーカー (Circuit Breaking)
* レートリミット (Rate Limiting)

これらの機能は、Kubernetes/Openshiftの上に構築されたあらゆる分散アプリケーションにとって重要です。ここでは、これからビルドおよびデプロイを行うマイクロサービスの異なる組み合わせを利用します。私たちのアプリケーションは前のラボで開発した _Coolstore microservice_ と呼ばれるもの(Catalog、Inventoryなど)で、 _Module 1_ または _Module 2_ で開発してOpenShiftクラスタにデプロイしたものです。

[WARNING]
====
モジュール 1 で _inventory_ と _catalog_ マイクロサービスを既にデプロイしている場合は、このステップをスキップして「1. サイドカーの自動インジェクションを有効にする」のセクションに進んでください。
====

**今日はモジュール1やモジュール2を実施していない場合** 、またはモジュール1やモジュール2を完全に終わらせていない場合は、CodeReady Workspaces Terminalで以下のシェルスクリプトを実行して、coolstoreアプリケーションとマイクロサービスをデプロイしてください:

次のスクリプトでinventoryサービスをデプロイします:

[source, shell, role="copypaste"]
----
sh $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/istio/scripts/deploy-inventory.sh {{ USER_ID }}  && \
sh $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/istio/scripts/deploy-catalog.sh {{ USER_ID }}
----

[WARNING]
====
OpenShiftではネットワークレイテンシのため、新しいビルドイメージの作成に時間がかかることがあります。そのため、 *Error from server (NotFound): services "catalog-springboot" not found* でカタログサービスのデプロイに失敗した場合には、少し待ってから、以下のコマンドを使ってもう一度試してみてください:
====

[source, sh, role="copypaste"]
----
sh $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/istio/scripts/deploy-inventory.sh {{ USER_ID }}  && \
sh $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/istio/scripts/deploy-catalog.sh {{ USER_ID }} 3m
----

コマンドが完了するのを待ちます。これで _inventory_ と _catalog_ コンポーネントがビルドされ、それぞれのネームスペースにデプロイされます。これらのコンポーネントはまだ自動的には Istio sidecar プロキシコンテナを取得しませんが、次のステップで追加します!

=== 1. サイドカーの自動インジェクションを有効にする

Red Hat OpenShift Service Mesh は、アプリケーションのポッド内のプロキシサイドカーに依存して、アプリケーションに Service Mesh 機能を提供します。サイドカーの自動インジェクションを有効にするか、手動で管理することができます。Red Hat は、プロジェクトにラベルを付ける必要のない、アノテーションを使用した自動インジェクションを推奨しています。これにより、デプロイ時にアプリケーションに Service Mesh 用の適切な設定が含まれていることが保証されます。この方法では、必要な権限が少なく、ビルダーポッドなどの他のOpenShift機能と競合しません。

[NOTE]
====
アップストリームバージョンの Istio では、プロジェクトにラベルを付けている場合、デフォルトでサイドカーがインジェクトされます。Red Hat OpenShift Service Meshでは、サイドカーが自動的にデプロイメントにインジェクトされるようにオプトインする必要があるため、プロジェクトにラベルを付ける必要はありません。これにより、サイドカーを必要としない場合（ビルドやデプロイポッドなど）にサイドカーをインジェクトすることを避けることができます。

webhook は、すべてのプロジェクトにデプロイするポッドの設定をチェックして、適切なアノテーションでインジェクションをオプトインしているかどうかを確認します。
====

==== `inventory` と `catalog` サービスが動作していることを確認

まず、以下のコマンドで catalog サービスと関連データベースが実行されていることを確認します:

[source,sh,role="copypaste"]
----
oc get pods -n {{USER_ID}}-catalog --field-selector status.phase=Running
----

2つのポッドが起動しているのがわかるはずです (1つはサービス用、もう1つはデータベース用):

[source,console]
----
NAME                         READY   STATUS    RESTARTS   AGE
catalog-database-1-p5hcd     1/1     Running   0          6m4s
catalog-springboot-1-jdrfs   1/1     Running   0          5m
----

inventoryに対しても同様にします:

[source,sh,role="copypaste"]
----
oc get pods -n {{USER_ID}}-inventory --field-selector status.phase=Running
----

再び、2つのポッドが実行されているのが確認できるはずです (1つはサービス用、もう1つはデータベース用):

[source,console]
----
NAME                         READY   STATUS    RESTARTS   AGE
inventory-1-87smf            1/1     Running   0          3m12s
inventory-database-1-6tvxs   1/1     Running   0          5m16s
----

==== サイドカーの追加

OpenShift Service Meshでは、デフォルトではアプリケーションがサービスメッシュの一部になることを「オプトイン」する必要があります。アプリを「オプトイン」するには、istio にサイドカーをアタッチしてアプリをメッシュに取り込むためのフラグであるアノテーションを追加する必要があります。

istioサイドカーをインジェクトするために必要なアノテーションを手動で追加するのではなく、以下のコマンドを実行して _inventory_ と _catalog_ のマイクロサービスとそれらに関連するデータベースにアノテーションを追加し、サイドカーのインジェクションがトリガされるようにします。

まず、データベースにアノテーションを設定して、再デプロイされるのを待ちます:
[source,sh,role="copypaste"]
----
oc patch dc/inventory-database -n {{USER_ID}}-inventory --type='json' -p '[{"op":"add","path":"/spec/template/metadata/annotations", "value": {"sidecar.istio.io/inject": "'"true"'"}}]' && \
oc patch dc/catalog-database -n {{USER_ID}}-catalog --type='json' -p '[{"op":"add","path":"/spec/template/metadata/annotations", "value": {"sidecar.istio.io/inject": "'"true"'"}}]' && \
oc rollout status -w dc/inventory-database -n {{USER_ID}}-inventory && \
oc rollout status -w dc/catalog-database -n {{USER_ID}}-catalog
----

1分くらいで終わるはずです。

[NOTE]
====
上の複雑そうなコマンドでは `oc patch` コマンドを使って Kubernetes オブジェクトをプログラマティックに編集しています。エディタで編集しても簡単だとは思いますが、YAMLは時々厄介なので、シンプルに実行できるようにしました!
====

次に、サービスにサイドカーを追加して、再デプロイされるのを待ちましょう:

[source,sh,role="copypaste"]
----
oc patch dc/inventory -n {{USER_ID}}-inventory --type='json' -p '[{"op":"add","path":"/spec/template/metadata/annotations", "value": {"sidecar.istio.io/inject": "'"true"'"}}]' && \
oc rollout latest dc/inventory -n {{USER_ID}}-inventory && \
oc patch dc/catalog-springboot -n {{USER_ID}}-catalog --type='json' -p '[{"op":"add","path":"/spec/template/metadata/annotations", "value": {"sidecar.istio.io/inject": "'"true"'"}}]' && \
oc rollout status -w dc/inventory -n {{USER_ID}}-inventory && \
oc rollout status -w dc/catalog-springboot -n {{USER_ID}}-catalog
----

これも1分程度で終了するはずです。終了したら、 `inventory-database` が2つのポッド( `READY` 列の `2/2` )で実行されていることを以下のコマンドで確認してください:

[source,sh,role="copypaste"]
----
oc get pods -n {{USER_ID}}-inventory --field-selector="status.phase=Running"
----

It should show:
次のように表示されるはずです:

[source,console,role="copypaste"]
----
NAME                         READY   STATUS    RESTARTS   AGE
inventory-2-2p9bx            2/2     Running   0          65s
inventory-database-2-4rzlh   2/2     Running   0          82s
----

Do the same for the catalog and confirm they also show the "2/2" pods running:
catalogサービスに対しても同様にして、「2/2」ポッドが動作していることを確認してください:

[source,sh,role="copypaste"]
----
oc get pods -n {{USER_ID}}-catalog --field-selector="status.phase=Running"
----

[source,console,role="copypaste"]
----
NAME                         READY   STATUS    RESTARTS   AGE
catalog-database-2-djmbd     2/2     Running   0          39m
catalog-springboot-2-x7cf6   2/2     Running   0          39m
----

[WARNING]
====
`inventory` と `catalog` サービスが認識されてメッシュに取り込まれるまでには、1～2分かかるかもしれません。
====

次に、catalogに着信トラフィックを送信する仮想サービスを作成してみましょう。 _catalog/rules_ ディレクトリ内の空の `catalog-default.yaml` ファイルを開き、CodeReady Workspacesを使用して以下の _VirtualService_ を空のファイルにコピーします:

[source,yaml, role="copypaste"]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog-default
spec:
  hosts:
  - "istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}"
  gateways:
  - {{USER_ID}}-bookinfo/bookinfo-gateway
  http:
    - match:
        - uri:
            exact: /services/products
        - uri:
            exact: /services/product
        - uri:
            exact: /
      route:
        - destination:
            host: catalog-springboot
            port:
              number: 8080
----

CodeReady Workspaces Terminalで以下のコマンドを実行します:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/catalog/rules/catalog-default.yaml -n {{ USER_ID }}-catalog
----

http://istio-ingressgateway-{{USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}[カタログサービスページ^] にアクセスし、以下のようになっていることを確認してください:

image::catalog-ui-gateway.png[catalog, 700]

[NOTE]
====
_istio ingress_ と _gateway_ と _virtual service_ を反映するのに数秒かかります。このページを開いたままにしておくと、 _Catalog UI browser_ がサービス間のトラフィックを（2秒ごとに）作成してくれるので、テストに便利です。
====

各ポッドに _side car_ がインジェクトできたかどうかを確認します。 https://kiali-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}/console/graph/namespaces/?edges=noEdgeLabels&graphType=versionedApp&namespaces={{ USER_ID }}-catalog%2C{{ USER_ID }}-inventory&unusedNodes=false&injectServiceNodes=true&duration=60&pi=15000&layout=dagre[Kiali Graph page^] にアクセスし、 *{{ USER_ID }}-inventory* および *{{ USER_ID }}-catalog* が選択された _Namespaces_ になっていることを確認します。その後、 _Display_ ドロップダウンで *Traffic Animation* を有効にすると、 _Catalog service_ から _Inventory service_ へのトラフィックの流れがアニメーションで表示されます:

image::kiali_graph_sidecar.png[istio, 700]

各ブランチに沿って、catalogとinventoryの両方のデータベースへのトラフィックとともに、catalogサービスへの着信トラフィックを見ることができます。これは私たちが期待するものと一致しています - catalogフロントエンドにアクセスすると、catalogバックエンドへの呼び出しが行われ、catalogバックエンドはinventoryにアクセスしてcatalogデータと結合し、表示用の結果を返します。

[NOTE]
====
時折、 _unknown_ や _PassthroughCluster_ 要素がグラフに表示されることがあります。これらはリアルタイムで行っている istio の設定変更によるもので、十分に待てば消えてしまいますが、このラボでは無視しても構いません。
====

=== 2. フォルトインジェクション (Fault Injection)

このステップでは、 *Fault Injection* を使用して、アプリケーション全体のエンドツーエンドの障害回復能力をテストする方法を説明します。障害回復ポリシーの構成が不適切な場合、重要なサービスが利用できなくなる可能性があります。不適切な構成の例としては、サービスコール間で互換性のないタイムアウトや制限されたタイムアウトがあります。

_Istio_ は、アプリケーション内のサービスで利用できる障害復旧機能のセットを提供しています。機能には以下のようなものがあります:

* **Timeouts**: 遅いサービスの待ち時間を最小限に抑えるためのタイムアウト
* **Bounded retries**: タイムアウトバジェットとリトライ間のジッターを可変にした制限付きリトライ
* **Limits** : アップストリームサービスへの同時接続数とリクエスト数の制限
* **Active (periodic) health checks**: ロードバランシングプールの各メンバーのアクティブな（定期的な）ヘルスチェック
* **Fine-grained circuit breakers**: 細粒度のサーキットブレーカー(パッシブヘルスチェック) - ロードバランシングプールのインスタンスごとに適用

これらの機能は、実行時にIstioのトラフィック管理ルールを介して動的に設定することができます。

アクティブとパッシブのヘルスチェックを組み合わせることで、不健全なサービスにアクセスする機会を最小限に抑えることができます。プラットフォームレベルのヘルスチェック（OpenShiftでの rediness/liveness プローブなど）と組み合わせることで、アプリケーションは、不健全なポッド/コンテナ/VMをサービスメッシュから迅速に取り除くことができ、リクエストの失敗やレイテンシへの影響を最小限に抑えることができます。

これらの機能を組み合わせることで、サービスメッシュは障害のあるノードを許容し、局部的な障害が他のノードに不安定性を連鎖させるのを防ぐことができます。

Istioは、TCP層でパケットを遅延させたり破損させたりすることで、（ポッドを殺すのではなく）ネットワークへのプロトコル固有の _fault injection_ を可能にします。

2種類のフォルトを注入することができます:

* _Delays_ (遅延)はタイミング障害です。ネットワーク遅延の増加やアップストリーム・サービスの過負荷を模倣します。
* _Aborts_ (アボート)はクラッシュ障害です。アップストリーム・サービスの障害を模倣します。アボートは通常、HTTP エラーコードまたは TCP 接続の失敗の形で表現されます。

アプリケーション内のマイクロサービスの耐障害性をテストするために、 _inventory_ サービスへのリクエストの *50%* にフォルトを注入し、時間内の半分でサービスが失敗したように見えるようにします (そして `HTTP 5xx` エラーを返します)。

_inventory/rules_ ディレクトリ内の空の `inventory-default.yaml` ファイルを開き、以下をファイルにコピーします:

[source,yaml,role="copypaste"]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: inventory-default
spec:
  hosts:
  - "istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}"
  gateways:
  - {{USER_ID}}-bookinfo/bookinfo-gateway
  http:
    - match:
        - uri:
            exact: /services/inventory
        - uri:
            exact: /
      route:
        - destination:
            host: inventory
            port:
              number: 8080
----

以下を実行して、以前に設定した catalog への直接ルートののゲートウェイを削除します:

[source,sh,role="copypaste"]
----
oc delete -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/catalog/rules/catalog-default.yaml -n {{ USER_ID }}-catalog
----

CodeReady Workspaces Terminal経由で以下のコマンドを実行して、inventoryサービスへのトラフィックを転送するVirtualServiceを新規に作成します:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/inventory/rules/inventory-default.yaml -n {{ USER_ID }}-inventory
----

さて、 http://istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}[CoolStore Inventory page^] にアクセスして、inventoryサービスが正常に動作するかどうかをテストしてみましょう。それでも _Coolstore Catalog_ が表示される場合は、kbd:[CTRL+F5]（Mac OSの場合はkbd:[Command+Shift+R]）でページをリロードして、 _Coolstore Inventory_ を表示させてください。

image::inventory-ui-gateway.png[fault-injection, 700]

_inventory_ マイクロサービスへのリクエストの *50%* にフォルト( _500 status_ )を注入してみましょう。以下のように _inventory-default.yaml_ を編集します。

_inventory/rules_ ディレクトリ内の空の `inventory-vs-fault.yaml` ファイルを開き、以下のコードをコピーしてください。

[source,yaml,role="copypaste"]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: inventory-fault
spec:
  hosts:
  - "istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}"
  gateways:
  - {{USER_ID}}-bookinfo/bookinfo-gateway
  http:
    - fault:
         abort:
           httpStatus: 500
           percentage:
             value: 50
      route:
        - destination:
            host: inventory
            port:
              number: 8080
----

新規に *inventory-fault VirtualService* を作成する前に、既存の inventory-default virtualService を削除する必要があります。CodeReady Workspaces Terminal で以下のコマンドを実行します:

[source,sh,role="copypaste"]
----
oc delete virtualservice/inventory-default -n {{ USER_ID }}-inventory
----

次に、このコマンドで新しい VirtualService を作成します:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/inventory/rules/inventory-vs-fault.yaml -n {{ USER_ID }}-inventory
----

http://istio-ingressgateway-{{USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}[CoolStore Inventoryページ^] にもう一度アクセスして、フォールトインジェクションが正常に動作しているかどうかを確認してみましょう。CoolStore Inventoryの *Status* (状態)が *DEAD* と *OK* の間で変化し続けていることがわかります。

image::inventory-dead-ok.png[fault-injection, 700]

https://kiali-{{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}/console/graph/namespaces/?edges=noEdgeLabels&graphType=versionedApp&namespaces={{ USER_ID }}-catalog%2C{{ USER_ID }}-inventory&unusedNodes=false&injectServiceNodes=true&duration=60&pi=15000&layout=dagre[Kiali Graph page^] を見ると、 _istio-ingressgateway_ からの `red` のトラフィックと、リクエストの約50%が _HTTP Traffic_ の右側に _5xx_ と表示されています。catalogとingress gatewayから同時に来るトラフィックがあるので、 _正確に_ 50%ではないかもしれませんが、時間の経過とともに50%に近づくでしょう。

[WARNING]
====
Kialiは "ルックバック" を行い、最後の1分間のトラフィックを記録/表示します。1分以内にグラフはクリアになり、あなたが探しているものだけが表示されます!
====

image::inventlry-vs-error-kiali.png[fault-injection,700]

それでは、`inventory` サービスに5秒の遅延を加えてみましょう。

_inventory/rules_ ディレクトリ内の空の `inventory-vs-fault-delay.yaml` ファイルを開き、以下のコードをコピーしてください:

[source,yaml,role="copypaste"]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: inventory-fault-delay
spec:
  hosts:
  - "istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}"
  gateways:
  - {{USER_ID}}-bookinfo/bookinfo-gateway
  http:
    - fault:
         delay:
           fixedDelay: 5s
           percentage:
             value: 100
      route:
        - destination:
            host: inventory
            port:
              number: 8080
----

CodeReady Workspaces Terminal で既存のインベントリの fault VirtualService を削除します:

[source,sh,role="copypaste"]
----
oc delete virtualservice/inventory-fault -n {{ USER_ID }}-inventory
----

次に、新しい virtualservice を作成します:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/inventory/rules/inventory-vs-fault-delay.yaml -n {{ USER_ID }}-inventory
----

先ほど開いた *Kiali Graph* に行くと、 _istio-ingressgateway_ からの `green` のトラフィックがinventoryサービスからのリクエストに対して遅延していることがわかります。この場合、 _Display_ の選択ボックスで *Traffic Animation* にチェックを入れる必要があることに注意してください。

[NOTE]
====
以前のフォルトインジェクションによる "red" トラフィックがまだ表示されているかもしれませんが、グラフの1分間のタイムウィンドウ（デフォルトのルックバック期間）が経過すると消えてしまいます。
====

image::inventlry-vs-delay-kiali.png[fault-injection,700]

「エッジ」( `istio-ingressgateway` と `inventory` の間の線) をクリックし、右側の _HTTP Request Response Time_ グラフの一番下までスクロールしてください。黒い _average_ データポイントにカーソルを合わせて、平均応答時間が予想通り約5000ms(5秒)であることを確認してください:

image::5sdelay.png[delay, 800]

Inventoryのフロントページが遅延を正しく処理できるならば、約5秒以内に読み込まれることが予想されます。ウェブページの応答時間を確認するには、IE、Chrome、FirefoxのDeveloper Toolsメニューを開き（典型的には kbd:[CTRL+SHIFT+I] 、Macの場合は kbd:[CMD+ALT+I] ）、 `Network` タブを選択し、inventoryのウェブページをリロードしてください。

You will see and feel that the webpage loads in about 5 seconds:
約5秒でウェブページが読み込まれるのを確認、体感してください:

image::inventory-webui-delay.png[Delay, 700]

次のステップに移る前に、ターミナルでこれらのコマンドを使用して、フォールトインジェクションをクリーンアップし、デフォルトの仮想サービスをもう一度設定します:

[source,sh,role="copypaste"]
----
oc delete virtualservice/inventory-fault-delay -n {{ USER_ID }}-inventory && \
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/inventory/rules/inventory-default.yaml -n {{ USER_ID }}-inventory
----

また、不要な負荷を避けるために、ブラウザのInventoryとCatalogサービスのタブを閉じ、CodeReadyのターミナルウィンドウを閉じてこのラボの最初に実行した無限の`for`ループを止めてください。

=== 3. サーキットブレーカーの有効化

このステップでは、 `Inventory` サービスへの呼び出しを保護するためにサーキットブレーカーを構成します。呼び出し回数が多いために `Inventory` サービスが過負荷になった場合、Istio はサービスインスタンスへの今後の呼び出しを制限して、サービスインスタンスが回復できるようにします。

サーキットブレーカーは、分散システムの重要なコンポーネントです。アップストリームのバックプレッシャーに迅速に対応して早めに失敗させることは、だいたいにおいて良い対応手段となります。Istioは、各アプリケーションを個別に設定してコード化する方法とは対照的に、ネットワークレベルでサーキットブレーカーの制限を強制します。

Istioは、サーキットブレイクのトリガーとなる様々なタイプの条件に対応しています:

* *Cluster maximum connections*: クラスタ最大接続数、Istioがクラスタ内のすべてのホストに確立する最大接続数

* *Cluster maximum pending requests*: クラスタの最大保留要求数、接続プール接続の準備ができている間にキューに入れられるリクエストの最大数

* *Cluster maximum requests*: クラスタ最大リクエスト数、クラスタ内のすべてのホストに対して、ある時点で未処理のリクエストの最大数。実際には、HTTP/1.1 クラスタは最大接続数サーキットブレーカーによって管理されているため、これは HTTP/2 クラスタに適用されます。

* *Cluster maximum active retries*: クラスタの最大アクティブリトライ数、クラスタ内のすべてのホストに対して、任意の時点で最大のリトライ数を指定します。一般的には、散発的な障害に対するリトライは許可されていても、全体のリトライ量が爆発して大規模なカスケード障害を引き起こすことがないように、積極的にサーキットブレイキングリトライを行うことをお勧めします。

[NOTE]
====
*HTTP2* は単一の接続を使用し、キューイングを行わない (常にマルチプレックス) ので、最大接続数と最大保留中のリクエスト数は適用されません。
====

各サーキットブレーキングの制限は、上流クラスタごと、優先度ごとに設定可能であり、追跡されます。これにより、分散システムの異なるコンポーネントを独立してチューニングし、異なる制限を持つことができます。詳細は https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/circuit_breaking[Envoyのサーキットブレーカー^] を参照してください。

ここでは、 *Inventoryサービス* への呼び出しにサーキットブレーカーを追加してみましょう。 _VirtualService_ オブジェクトを使用する代わりに、Istioのサーキットブレーカーは _DestinationRule_ オブジェクトとして定義されています。DestinationRule は、ルーティングが発生した後に、サービスの対象となるトラフィックに適用されるポリシーを定義します。これらのルールは、負荷分散の設定、サイドカーからの接続プールサイズ、負荷分散プールから不健全なホストを検出して退避させるための外れ値検出の設定を指定します。

_inventory/rules_ ディレクトリ内の空の *inventory-cb.yaml* ファイルを開き、Inventoryサービスの呼び出し時にサーキットブレイキングを有効にするためにこのコードを追加します:

[source,yaml,role="copypaste"]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: inventory-cb
spec:
  host: inventory
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 1
      http:
        http1MaxPendingRequests: 1
        maxRequestsPerConnection: 1
----

CodeReady Workspacesターミナルから以下のコマンドを実行して、ルールを作成します:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/inventory/rules/inventory-cb.yaml -n {{ USER_ID }}-inventory
----

Inventoryサービスの最大接続数を 1 に、最大保留中のリクエストを 1 に設定しました。 したがって、短期間に 2 つ以上のリクエストをinventoryサービスに送信すると、1 つは許可され、1 つは保留され、保留中のリクエストが処理されるまで追加のリクエストは拒否されます。さらに、サーバーエラー（HTTP 5xx）を返すホストを検出し、15分間、負荷分散プールからポッドをイジェクトします。それぞれの設定パラメータがどのようなものかは、 https://istio.io/docs/tasks/traffic-management/circuit-breaking[Istio spec^] を参照してください。

=== 4. サービスの過負荷

我々は _siege_ というユーティリティを使って、アプリケーションに複数の同時リクエストを送信し、サーキットブレーカーがキックインして開通するのを目撃しましょう。

これを実行して、CodeReady Workspaces Terminal でゲートウェイ URL に同時にアクセスしようとする多数のユーザーをシミュレートします。

[source,sh,role="copypaste"]
----
siege --verbose --time=1M --concurrent=10 'http://istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}'
----

これは1分間実行され、 `[error] Failed to make an SSL connection. 5` のようなエラーが出ることがありますが、これはサーキットブレーカーがトリップしてリクエストの洪水がサービスに届くのを止めていることを示しています。

これを確認するには、 https://grafana-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}/d/LJ_uJAvmk/istio-service-dashboard?orgId=1&refresh=10s&var-service=inventory.{{ USER_ID }}-inventory.svc.cluster.local&var-srcns=All&var-srcwl=All&var-dstns=All&var-dstwl=All[Istio Service Dashboard^] でGrafanaを開き、Inventoryサービスの _Client Success Rate(non-5xx responses)_ がもはや100%になっていないことを確認します:

[NOTE]
====
Prometheusメトリクスの特性(准リアルタイム性)やGrafanaのリフレッシュ期間、一般的なネットワーク遅延のため、サーキットブレーカーの痕跡がGrafanaダッシュボードに表示されるまでに10～20秒かかることがあります。また、 `siege` コマンドを再実行して、より多くの障害を強制的に発生させることもできます。
====

image::inventory-circuit-breaker-grafana.png[circuit-breaker, 700]

これがサービスへのリクエスト数を制限するサーキットブレーカーの動きです。実際には(リクエスト数の制限値は)もっと高くなるでしょう

また、アニメーションの中でサーキットブレーカーが `HTTP 503` エラーを発生させているのを見ることができます:

image::inventory-circuit-breaker-kiali.png[circuit-breaker, 700]

実際には、これらの `503` は、アップストリームのフォールバックを引き起こしつつ、過負荷のサービスに回復する機会を与えていることを意味します。

次のステップに進む前に、以下のコマンドで既存のdestinationrule、virtural service、gatewayをクリアします。

[source,sh,role="copypaste"]
----
oc delete destinationrule/inventory-cb -n {{ USER_ID }}-inventory && \
oc delete virtualservice/inventory-default -n {{ USER_ID }}-inventory && \
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/catalog/rules/catalog-default.yaml -n {{ USER_ID }}-catalog
----

=== 6. シングルサインオンによる認証の有効化

このステップでは、認証を有効にする方法を学習します。 _Catalog_ エンドポイントをセキュアにします。Red Hat Runtimesの一部であるRed Hat Single Sign Onを利用してJWTを適用します。

参考情報:

* https://en.wikipedia.org/wiki/JSON_Web_Token[JSON Web Token(JWT)^]
* https://access.redhat.com/products/red-hat-single-sign-on[Red Hat Single Sign-On^]
* https://www.redhat.com/en/products/application-runtimes[Red Hat Runtimes^]

サービスメッシュ内のトラフィックのサービス認証を可能にする *Red Hat Single Sign-On (RH-SSO)* を導入してみましょう。

_Red Hat Single Sign-On (RH-SSO)_ は、 *Keycloak* プロジェクトをベースにしており、 *SAML 2.0、OpenID Connect、OAuth 2.0* などの一般的な標準に基づいたWebシングルサインオン(SSO)機能を提供することで、Webアプリケーションのセキュリティを確保することができます。RH-SSOサーバは、SAMLまたはOpenID ConnectベースのIdentity Providerとして機能し、標準ベースのトークンを介して、アイデンティティ情報やアプリケーションと企業のユーザーディレクトリまたはサードパーティのSSOプロバイダを仲介することができます。主な機能は以下の通りです:

* *Authentication Server* (認証サーバ) - スタンドアロン SAML または OpenID Connect ベースの Identiry Provier として動作
* *User Federation* - ユーザー情報のソースとして LDAP サーバーおよび Microsoft Active Directory を認定
* *Identity Brokering* - ID ソースとして、主要なソーシャルネットワークを含むサードパーティの Identity Provider と統合
* *REST APIs and Administration GUI* - 使いやすい管理GUIとREST APIを使用して、ユーザーフェデレーション、ロールマッピング、クライアントアプリケーションを指定

新規プロジェクトでRH-SSOをデプロイします。 {{ CONSOLE_URL }}/topology/ns/{{ USER_ID }}-catalog[トポロジービュー^] に移動し、 *Create Project* をクリックします:

image::create_project_topology.png[rhsso, 500]

以下の名前を入力し、 *Create* をクリックします:

 * Name: `{{ USER_ID}}-rhsso`

image::create_project_popup.png[rhsso, 500]

トポロジービューの *From Catalog* をクリックします:

image::create_project_fc.png[rhsso, 700]

検索ボックスに `ccn-sso` と入力し、 *CCN + Red Hat Single Sign-On 7.4 on OpenJDK + PostgreSQL* カタログをクリックします。その後、 *Instantiate Template* をクリックします:

image::catalog_rhsso.png[rhsso, 700]

以下の文字列を入力し、他の文字列はデフォルトのままにしておきます。 *作成* をクリックします:

 * RH-SSO Administrator Username: `admin`
 * RH-SSO Administrator Password: `admin`
 * RH-SSO Realm: `istio`
 * RH-SSO Service Username: `auth{{ USER_ID}}`
 * RH-SSO Service Password: `{{ OPENSHIFT_USER_PASSWORD }}`

image::catalog_rhsso_detail.png[rhsso, 700]

CodeReady Workspaces Terminalから以下のラベルを追加します:

[source,sh,role="copypaste"]
----
oc project {{ USER_ID}}-rhsso && \
oc label dc/sso app.openshift.io/runtime=sso && \
oc label dc/sso-postgresql app.openshift.io/runtime=postgresql --overwrite && \
oc label dc/sso-postgresql app.kubernetes.io/part-of=sso --overwrite && \
oc label dc/sso app.kubernetes.io/part-of=sso --overwrite && \
oc annotate dc/sso-postgresql app.openshift.io/connects-to=sso --overwrite
----

{{console_url }}/topology/ns/{{user_id }}-rhsso[トポロジービュー^] に戻ります:

image::rhsso-topology.png[sso, 700]

これが終わったら（1～2分かかるかもしれません）、 https://secure-sso-{{ USER_ID }}-rhsso.{{ ROUTE_SUBDOMAIN}}[Secure SSO Route^] をクリックして、以下のようにRH-SSOのWebコンソールにアクセスします:

image::rhsso_landing_page.png[sso, 700]

_Administration Console_ をクリックして *Istio* Reamを設定し、先ほど使用したユーザ名とパスワードを入力します:

* Username or email: `admin`
* Password: `admin`

image::rhsso_admin_login.png[sso, 700]

_Istio Realm_ の一般的な情報が表示されます。 *Login* タブをクリックして、 _Require SSL_ を _none_ に設定して非選択(swich off)にしてから、 *Save* をクリックしてください。

image::rhsso_istio_realm.png[sso, 700]

[NOTE]
====
Red Hat Single Sign-On は初回起動時に自己署名証明書を生成します。自己署名付き証明書ではIstioでの認証がうまくいかないので、Istio認証のテストにSSLを使わないように変更します。
====

次に、 _Istio_ レルム内の信頼されたブラウザアプリとWebサービスのための新しいRH-SSO _client_ を作成します。左メニューの *Clients* に移動し、 *Create* をクリックします。

image::rhsso_clients.png[sso, 700]

_Client ID_ フィールドに `ccn-cli` と入力し、 *Save* をクリックします。

image::rhsso_clients_create.png[sso, 700]

次の画面では、 *Settings* タブの詳細が表示されます。あなたがする必要がある唯一のことは、クライアントのために成功したログインまたはログアウト後に使用することができる _Valid Redirect URI_ を入力することです。

[source,sh,role="copypaste"]
----
http://istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}/*
----

image::rhsso_clients_settings.png[sso, 700]

*Save* をクリックするのを忘れずに!

では、資格情報に割り当てられるロールを定義してみましょう。 *ccn_auth* という名前のシンプルなロールを作成します。 左メニューの *Roles* に移動し、 _Add Role_ をクリックします。

image::rhsso_roles.png[sso, 700]

_Role Name_ フィールドに `ccn_auth` と入力し、*Save* をクリックします。

image::rhsso_roles_create.png[sso, 700]

次に、 _authuser_ のパスワードポリシーを更新してみましょう。

左側メニューの *Users* に移動し、 *View all users* をクリックします。

image::rhsso_users.png[sso, 700]

`auth{{ USER_ID }}` IDをクリックすると、詳細、属性、資格情報、ロールマッピング、グループ、コンテンツ、セッションなどの情報が表示されます。このステップでは詳細を更新する必要はありません。

image::rhsso_istio_users_details.png[sso, 700]

*Credentials* タブに移動し、以下の変数を入力します:

* New Password: `{{ OPENSHIFT_USER_PASSWORD }}`
* Password Confirmation: `{{ OPENSHIFT_USER_PASSWORD }}`
* Temporary: `OFF`

auth{{ USER_ID }} に最初の認証時にパスワードを変更させたくない場合は、必ず *Temporary* フラグをオフにしてください。

*Reset Password* をクリックします。

image::rhsso_users_credentials.png[sso, 700]

次に、ポップアップウィンドウで *Change password* をクリックします。

image::rhsso_users_change_pwd.png[sso, 700]

*Role Mappings* タブに進み、 _Add selected >_ をクリックして *ccn_auth* ロールを割り当てます。

image::rhsso_rolemapping.png[sso, 700]

_Assigned Roles_ ボックスでccn_authロールを確認します。

image::rhsso_rolemapping_assigned.png[sso, 700]

よくできました、あなたはカスタムレルム、ユーザー、およびロールでRH-SSOを有効にしました!

==== サービス名の変更

OpenShift Service Meshの今後のバージョンでは、Istioの新しいバージョンでは、 `http` のように https://istio.io/docs/ops/configuration/traffic-management/protocol-selection/#automatic-protocol-selection[プロトコルの自動検出^] ができるようになります。しかし今のところは、認証や認可ポリシーの適用などの高度なことができるようにするためには、Kubernetesのサービス名にプロトコル名を明示的に含める必要があります。そのためには、以下のコマンドを実行して、catalogとinventoryの両方のサービス名を更新します:

[source,sh,role="copypaste"]
----
oc patch -n {{ USER_ID }}-catalog svc/catalog-springboot -p '{"spec": {"ports":[{"port": 8080, "name": "http"}, {"port": 8443, "name": "https"}]}}'
----

Istioに話を戻して、JSON Web Tokens (JWT)と https://openid.net/connect/[OIDC^] 認証フローを使って、ユーザインタフェースの認証ポリシーを作成してみましょう。

CodeReadyで、 _catalog/rules_ ディレクトリ内の空の *ccn-auth-config.yml* ファイルを開き、以下のポリシーをコピーします。

[source,yaml,role="copypaste"]
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: auth-policy
  namespace: {{ USER_ID }}-catalog
spec:
  targets:
  - name: catalog-springboot
  origins:
  - jwt:
      issuer: http://sso-{{ USER_ID }}-rhsso.{{ ROUTE_SUBDOMAIN }}/auth/realms/istio
      jwks_uri: http://sso-{{ USER_ID }}-rhsso.{{ ROUTE_SUBDOMAIN }}/auth/realms/istio/protocol/openid-connect/certs
  principalBinding: USE_ORIGIN
----

Istioでポリシーを作成する際には、上記のように以下のフィールドを使用します。説明は以下の通りです:

* *issuer* - JWTを発行した発行者を識別します。 https://tools.ietf.org/html/rfc7519#section-4.1.1[issuer^] を参照のこと。通常はURLまたは電子メール・アドレス。
* *jwksUri* - JWTの署名を検証するために設定されたプロバイダの公開鍵のURL。
* *audiences* - アクセスが許可されているJWT https://tools.ietf.org/html/rfc7519#section-4.1.3[audiences^] のリスト。これらのオーディエンスのいずれかを含むJWTが受け入れられます。

次に、CodeReady Workspaces Terminalで以下のocコマンドを実行して、このオブジェクトを作成します:

[source,sh,role="copypaste"]
----
oc create -f $CHE_PROJECTS_ROOT/cloud-native-workshop-v2m3-labs/catalog/rules/ccn-auth-config.yaml -n {{ USER_ID }}-catalog
----

これでRH-SSOで認証されないとcatalogサービスにアクセスできなくなりました。CodeReady Workspaces Terminalで以下のcurlコマンドで確認します:

[source,sh,role="copypaste"]
----
curl -i http://istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}
----

`HTTP 401 Unauthorized` と `Origin authentication failed.` のメッセージが表示されるはずです。

このレスポンスは期待どおりです。なぜなら、RH-SSOで有効なJWTトークンでユーザが識別されていないためです。Istio Mixerの認証ポリシーの初期化には、通常、`5～10秒`かかります。この後、しばらくの間はポリシーがキャッシュされるので、認証ポリシーの適用は高速に行われます。

image::rhsso_call_catalog_noauth.png[sso, 900]

正しいトークンを生成するには、CodeReady Workspaces Terminalで次の `curl` リクエストを実行します。このコマンドは、RH-SSOから出力されたAuthorizationトークンを *TOKEN* という環境変数に格納します。

[source,sh,role="copypaste"]
----
export TOKEN=$( curl -X POST 'http://sso-{{ USER_ID }}-rhsso.{{ ROUTE_SUBDOMAIN }}/auth/realms/istio/protocol/openid-connect/token' \
 -H "Content-Type: application/x-www-form-urlencoded" \
 -d "username=auth{{ USER_ID }}" \
 -d 'password={{ OPENSHIFT_USER_PASSWORD }}' \
 -d 'grant_type=password' \
 -d 'client_id=ccn-cli' | jq -r '.access_token')  && echo $TOKEN;
----

トークンを生成したら、CodeReady Workspaces Terminalでトークンを指定して以下のcurlコマンドを再実行します:

[source,sh,role="copypaste"]
----
curl -s -H "Authorization: Bearer $TOKEN" http://istio-ingressgateway-{{ USER_ID }}-istio-system.{{ ROUTE_SUBDOMAIN }}/services/products | jq
----

以下のような期待される出力が表示されるはずです:

[source,json]
----
...
 {
    "itemId": "444435",
    "name": "Quarkus twill cap",
    "desc": "",
    "price": 13,
    "quantity": 600
  },
  {
    "itemId": "444437",
    "name": "Nanobloc Universal Webcam Cover",
    "desc": "",
    "price": 2.75,
    "quantity": 230
  }
]
----

おめでとうございます。IstioとRH-SSOを統合して、アプリケーションを全く変更することなく、カタログサービスへのサービス・メッシュ・トラフィックを保護することができました。Istioは、Keycloakを使用して、サービス間の通話（「east-west」トラフィックとも呼ばれる）を認証することができます。

フロントエンドのウェブアプリケーションからのトラフィックなどの「north-south」トラフィックについては、RH-SSOは、Spring BootやJBoss EAPなどのアプリ用にhttps://access.redhat.com/documentation/en-us/red_hat_single_sign-on/7.3/html/securing_applications_and_services_guide/openid_connect_3[各種アダプタ^] を提供しており、アプリがRH-SSOに対して認証を行うように設定できます。また、Quarkusは、これらのタイプのアプリ用のMicroProfile JWTとKeycloakアダプタも提供しています。詳細は、https://quarkus.io/guides[Quarkus Guides^] を参照してください。

また、*Red Hat* は https://access.redhat.com/products/quarkus[Red Hat build of Quarkus(RHBQ)^] を利用して、Quarkusの主要なバージョンのサポートやメンテナンスを定められた期間にわたって提供しています。今回のワークショップでは、RHBQを使ってクラウドネイティブのマイクロサービスを開発します。 https://access.redhat.com/documentation/en-us/red_hat_build_of_quarkus[RHBQについて詳しくはこちら^] 。 https://www.redhat.com/en/products/runtimes[Red Hat Runtimes^] に含まれるクラウドネイティブランタイムの一つです。

Red Hat SSO と istio を組み合わせると、サービスメッシュ内のトラフィックとメッシュを出入りするトラフィックを適切に認証できるようになります。

=== まとめ

このシナリオでは、最新の分散型アプリケーションに必要な機能の多くを実装するために Istio を使用しました。

Istioは、サービスコードの変更を必要とせずに、デプロイされたサービス間のネットワークにロードバランシング、サービス間認証、監視などを簡単に備える方法を提供します。Istioのコントロール・プレーン機能を使用して設定・管理されたマイクロサービス間のすべてのネットワーク通信を遮断する特別なサイドカー・プロキシを環境全体に配備することで、サービスにIstioのサポートを追加できます。

コンテナやOpenShiftのようなコンテナ・オーケストレーション・プラットフォームのような技術は、分散アプリケーションのデプロイメントを非常にうまく解決していますが、分散マイクロサービス・アプリケーションを十分に活用するために必要なサービス通信への対応にはまだ追いついていません。Istioを使用すると、これらの問題の多くをビジネスロジックの外で解決することができ、開発者はインフラストラクチャに属する問題から解放されます。

*おめでとうございます!*
